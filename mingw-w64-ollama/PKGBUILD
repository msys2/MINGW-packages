_realname=ollama
pkgbase=mingw-w64-${_realname}
pkgname="${MINGW_PACKAGE_PREFIX}-${_realname}"
pkgdesc='Create, run and share large language models (LLMs) (mingw-w64)'
pkgver=0.1.45
pkgrel=1
arch=(any)
mingw_arch=('mingw64' 'ucrt64' 'clang64' 'clangarm64')
msys2_repository_url='https://github.com/ollama/ollama'
url='https://ollama.com'
license=('spdx:MIT')
makedepends=(git
             ${MINGW_PACKAGE_PREFIX}-cmake
             ${MINGW_PACKAGE_PREFIX}-go
             ${MINGW_PACKAGE_PREFIX}-cc
             ${MINGW_PACKAGE_PREFIX}-ninja)
source=(git+https://github.com/ollama/ollama/#tag=v$pkgver
        0001-use-ninja-cc.patch
        0002-remove-cmake-arch.patch
        0003-missing-main.patch
)
b2sums=('109eee6c42771c3fb3170fbf1602c624c051b56e1f74464a6f1f3ad22ba7b78130b1e295ca912b3c94a0531b12a6111abe856a2f2e8ba1a6ad570a9f4a78bfcc'
        'f7d0879545bcec8aa3d1dbeed5afc6fe65992f34737e0739c7c8debae53e84688b9f072928e90eedd322339945f248ba0f11cedc2e8cb90177cc3a06316b1bad'
        'd56b6caf68818b6567ceb006d439f8f24a1bc8420cb520cb6d7bd5d2ff45423d94127fa5bc890625674b04771b94171a66189e450cac1e03b4fe9243166653bf'
        '013f57a5193142509d20c2fc0053442291f067d8993293ca77760f712959910313c7210027c49ef206288e7fe0299715f071a6c99f46071288617ff511351818')

apply_patch_with_msg() {
  for _patch in "$@"
  do
    msg2 "Applying ${_patch}"
    patch -Nbp1 -i "${srcdir}/${_patch}"
  done
}

prepare() {
  cd $_realname

  apply_patch_with_msg \
    ../0001-use-ninja-cc.patch \
    ../0002-remove-cmake-arch.patch \
    ../0003-missing-main.patch

  if check_option "debug" "n"; then
    sed -i 's,var mode string = gin.DebugMode,var mode string = gin.ReleaseMode,g' server/routes.go
  fi  
}

build() {
  cd $_realname
  export CGO_CFLAGS="$CFLAGS" CGO_CPPFLAGS="$CPPFLAGS" CGO_CXXFLAGS="$CXXFLAGS" CGO_LDFLAGS="$LDFLAGS"
  export GOPROXY=direct 
  export GOROOT=${MINGW_PREFIX}/lib/go
  export GOPATH=${MINGW_PREFIX}/

  if [[ ${CARCH} == aarch64 ]]; then
    # See https://github.com/ggerganov/llama.cpp/blob/4ee7a17d09cda02a76f865dcc73237f3b9a09fac/cmake/arm64-windows-llvm.cmake#L12
    local flags="-march=armv8.2-a -fvectorize -ffp-model=fast"

    export CMAKE_C_FLAGS_INIT="$flags"
    export CMAKE_CXX_FLAGS_INIT="$flags"
  fi
  
  go generate ./...
  go build -buildmode=pie -trimpath -mod=readonly -modcacherw -ldflags=-linkmode=external \
    -ldflags=-buildid='' -ldflags="-X=github.com/ollama/ollama/version.Version=$pkgver" -o "${srcdir}/build-${MSYSTEM}/"
}

check() {
  $srcdir/build-$MSYSTEM/ollama --version >/dev/null

  cd $_realname
  go test .
}

package() {
  cd $srcdir/build-$MSYSTEM

  mkdir -p "${pkgdir}${MINGW_PREFIX}/bin"
  install ollama.exe -t"${pkgdir}${MINGW_PREFIX}/bin"

  local runners=("cpu")
  local dist_arch=$([[ ${CARCH} == aarch64 ]] && echo "arm64" || echo "amd64")

  # ARM64 only supports 'cpu' which has NEON enabled by default
  if [[ ${CARCH} != aarch64 ]]; then
    runners+=("cpu_avx" "cpu_avx2")
  fi

  for runner in "${runners[@]}"
  do
    install -D "$srcdir/$_realname/dist/windows-$dist_arch/ollama_runners/$runner/libllama.dll" "${pkgdir}${MINGW_PREFIX}/bin/ollama_runners/$runner/libllama.dll"
    install -D "$srcdir/$_realname/dist/windows-$dist_arch/ollama_runners/$runner/ollama_llama_server.exe" "${pkgdir}${MINGW_PREFIX}/bin/ollama_runners/$runner/ollama_llama_server.exe"
  done
}
